.. meta::
   :description: Learn how to use ROCm for AI.
   :keywords: ROCm, AI, machine learning, LLM, usage, tutorial

**************************
Use ROCm for AI
**************************

ROCmâ„¢ is an open-source software platform that enables high-performance computing and machine learning applications. It features the ability to accelerate training, fine-tuning, and inference for AI application development. With ROCm, you can access the full power of AMD GPUs, which can significantly improve the performance and efficiency of AI workloads.

You can use ROCm to perform distributed training, which enables you to train models across multiple GPUs or nodes simultaneously. Additionally, ROCm supports mixed-precision training, which can help reduce the memory and compute requirements of training workloads. For fine-tuning, ROCm provides access to various algorithms and optimization techniques. In terms of inference, ROCm provides several techniques that can help you optimize your models for deployment, such as quantization, GEMM tuning, and optimization with composable kernel.
 
Overall, ROCm can be used to improve the performance and efficiency of your AI applications. With its training, fine-tuning, and inference support, ROCm provides a complete solution for optimizing AI workflows and achieving the optimum results possible on AMD GPUs. 

For AI tutorials in Jupyter Notebook format, see `Tutorials for AI developers <https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/>`_.

In this guide, you'll learn how to use ROCm for AI:

- :doc:`Training <training/index>`

- :doc:`Fine-tuning LLMs <fine-tuning/index>`

- :doc:`Inference <inference/index>`

- :doc:`Inference optimization <inference-optimization/index>`


To learn about ROCm for HPC applications and scientific computing, see
:doc:`../rocm-for-hpc/index`.
